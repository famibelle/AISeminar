{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk\n",
    "# %pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original :\n",
      " \n",
      "Maître Corbeau, sur un arbre perché,\n",
      "Tenait en son bec un fromage.\n",
      "Maître Renard, par l’odeur alléché,\n",
      "Lui tint à peu près ce langage :\n",
      "« Hé ! bonjour, Monsieur du Corbeau.\n",
      "Que vous êtes joli ! que vous me semblez beau !\n",
      "Sans mentir, si votre ramage\n",
      "Se rapporte à votre plumage,\n",
      "Vous êtes le Phénix des hôtes de ces bois. »\n",
      "A ces mots le Corbeau ne se sent pas de joie ;\n",
      "Et pour montrer sa belle voix,\n",
      "Il ouvre un large bec, laisse tomber sa proie.\n",
      "Le Renard s’en saisit, et dit : « Mon bon Monsieur,\n",
      "Apprenez que tout flatteur\n",
      "Vit aux dépens de celui qui l’écoute :\n",
      "Cette leçon vaut bien un fromage, sans doute. »\n",
      "Le Corbeau, honteux et confus,\n",
      "Jura, mais un peu tard, qu’on ne l’y prendrait plus.\n",
      "\n",
      "Jean de La Fontaine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemple de texte\n",
    "text = \"\"\"\n",
    "Maître Corbeau, sur un arbre perché,\n",
    "Tenait en son bec un fromage.\n",
    "Maître Renard, par l’odeur alléché,\n",
    "Lui tint à peu près ce langage :\n",
    "« Hé ! bonjour, Monsieur du Corbeau.\n",
    "Que vous êtes joli ! que vous me semblez beau !\n",
    "Sans mentir, si votre ramage\n",
    "Se rapporte à votre plumage,\n",
    "Vous êtes le Phénix des hôtes de ces bois. »\n",
    "A ces mots le Corbeau ne se sent pas de joie ;\n",
    "Et pour montrer sa belle voix,\n",
    "Il ouvre un large bec, laisse tomber sa proie.\n",
    "Le Renard s’en saisit, et dit : « Mon bon Monsieur,\n",
    "Apprenez que tout flatteur\n",
    "Vit aux dépens de celui qui l’écoute :\n",
    "Cette leçon vaut bien un fromage, sans doute. »\n",
    "Le Corbeau, honteux et confus,\n",
    "Jura, mais un peu tard, qu’on ne l’y prendrait plus.\n",
    "\n",
    "Jean de La Fontaine\n",
    "\"\"\"\n",
    "\n",
    "print(\"Texte original :\\n\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : Tokenisation par caractère"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens (caractères) : ['\\n', 'M', 'a', 'î', 't', 'r', 'e', ' ', 'C', 'o', 'r', 'b', 'e', 'a', 'u', ',', ' ', 's', 'u', 'r', ' ', 'u', 'n', ' ', 'a', 'r', 'b', 'r', 'e', ' ', 'p', 'e', 'r', 'c', 'h', 'é', ',', '\\n', 'T', 'e', 'n', 'a', 'i', 't', ' ', 'e', 'n', ' ', 's', 'o']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation par caractères\n",
    "char_tokens = list(text)\n",
    "print(\"\\nTokens (caractères) :\", char_tokens[:50])  # Affiche les 50 premiers tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : Tokenisation par mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens avec NLTK : ['Maître', 'Corbeau', ',', 'sur', 'un', 'arbre', 'perché', ',', 'Tenait', 'en', 'son', 'bec', 'un', 'fromage', '.', 'Maître', 'Renard', ',', 'par', 'l', '’', 'odeur', 'alléché', ',', 'Lui', 'tint', 'à', 'peu', 'près', 'ce', 'langage', ':', '«', 'Hé', '!', 'bonjour', ',', 'Monsieur', 'du', 'Corbeau', '.', 'Que', 'vous', 'êtes', 'joli', '!', 'que', 'vous', 'me', 'semblez', 'beau', '!', 'Sans', 'mentir', ',', 'si', 'votre', 'ramage', 'Se', 'rapporte', 'à', 'votre', 'plumage', ',', 'Vous', 'êtes', 'le', 'Phénix', 'des', 'hôtes', 'de', 'ces', 'bois', '.', '»', 'A', 'ces', 'mots', 'le', 'Corbeau', 'ne', 'se', 'sent', 'pas', 'de', 'joie', ';', 'Et', 'pour', 'montrer', 'sa', 'belle', 'voix', ',', 'Il', 'ouvre', 'un', 'large', 'bec', ',', 'laisse', 'tomber', 'sa', 'proie', '.', 'Le', 'Renard', 's', '’', 'en', 'saisit', ',', 'et', 'dit', ':', '«', 'Mon', 'bon', 'Monsieur', ',', 'Apprenez', 'que', 'tout', 'flatteur', 'Vit', 'aux', 'dépens', 'de', 'celui', 'qui', 'l', '’', 'écoute', ':', 'Cette', 'leçon', 'vaut', 'bien', 'un', 'fromage', ',', 'sans', 'doute', '.', '»', 'Le', 'Corbeau', ',', 'honteux', 'et', 'confus', ',', 'Jura', ',', 'mais', 'un', 'peu', 'tard', ',', 'qu', '’', 'on', 'ne', 'l', '’', 'y', 'prendrait', 'plus', '.', 'Jean', 'de', 'La', 'Fontaine']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation avec NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens_nltk = word_tokenize(text)\n",
    "\n",
    "print(\"Tokens avec NLTK :\", tokens_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : Tokenisation en sous-mots avec WordPiece (Hugging Face Tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6bcce9efa94799b141567d85f22694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b2ad8bf22d4e758f6e1ab15879b566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbe8ffbac644738b6941f8948eb8c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ae3332f9444e2a97d49ff576f4ef3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens avec WordPiece : ['mai', '##tre', 'co', '##rb', '##eau', ',', 'sur', 'un', 'ar', '##bre', 'perch', '##e', ',', 'ten', '##ai', '##t', 'en', 'son', 'be', '##c', 'un', 'from', '##age', '.', 'mai', '##tre', 'ren', '##ard', ',', 'par', 'l', '’', 'ode', '##ur', 'all', '##eche', ',', 'lu', '##i', 'tin', '##t', 'a', 'pe', '##u', 'pre', '##s', 'ce', 'lang', '##age', ':', '«', 'he', '!', 'bon', '##jou', '##r', ',', 'monsieur', 'du', 'co', '##rb', '##eau', '.', 'que', 'vo', '##us', 'et', '##es', 'jo', '##li', '!', 'que', 'vo', '##us', 'me', 'se', '##mble', '##z', 'beau', '!', 'sans', 'men', '##ti', '##r', ',', 'si', 'vo', '##tre', 'rama', '##ge', 'se', 'rap', '##port', '##e', 'a', 'vo', '##tre', 'plumage', ',', 'vo', '##us', 'et', '##es', 'le', 'ph', '##eni', '##x', 'des', 'hot', '##es', 'de', 'ce', '##s', 'bois', '.', '»', 'a', 'ce', '##s', 'mo', '##ts', 'le', 'co', '##rb', '##eau', 'ne', 'se', 'sent', 'pas', 'de', 'jo', '##ie', ';', 'et', 'pour', 'mont', '##rer', 'sa', 'belle', 'vo', '##ix', ',', 'il', 'ou', '##vre', 'un', 'large', 'be', '##c', ',', 'lai', '##sse', 'tomb', '##er', 'sa', 'pro', '##ie', '.', 'le', 'ren', '##ard', 's', '’', 'en', 'sai', '##sit', ',', 'et', 'di', '##t', ':', '«', 'mon', 'bon', 'monsieur', ',', 'app', '##ren', '##ez', 'que', 'to', '##ut', 'flat', '##te', '##ur', 'vi', '##t', 'aux', 'de', '##pen', '##s', 'de', 'ce', '##lu', '##i', 'qui', 'l', '’', 'eco', '##ute', ':', 'ce', '##tte', 'le', '##con', 'va', '##ut', 'bien', 'un', 'from', '##age', ',', 'sans', 'do', '##ute', '.', '»', 'le', 'co', '##rb', '##eau', ',', 'hon', '##te', '##ux', 'et', 'con', '##fus', ',', 'ju', '##ra', ',', 'mai', '##s', 'un', 'pe', '##u', 'tar', '##d', ',', 'qu', '’', 'on', 'ne', 'l', '’', 'y', 'pre', '##ndra', '##it', 'plus', '.', 'jean', 'de', 'la', 'fontaine']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Charger un tokenizer pré-entraîné basé sur WordPiece (ex : BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenisation avec WordPiece\n",
    "tokens_wordpiece = tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Tokens avec WordPiece :\", tokens_wordpiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens avec WordPiece : ['je', 'sui', '##s', 'en', 'train', 'de', 'sui', '##vre', 'une', 'formation', 'sur', 'l', \"'\", 'intelligence', 'art', '##ific', '##iel', '##le']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation avec WordPiece\n",
    "tokens_wordpiece = tokenizer.tokenize(\"Je suis en train de suivre une formation sur l'Intelligence Artificielle\")\n",
    "\n",
    "print(\"Tokens avec WordPiece :\", tokens_wordpiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens avec WordPiece : ['ten', '##ai', '##t', 'en', 'son', 'be', '##c', 'un', 'from', '##age', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_wordpiece = tokenizer.tokenize(\"Tenait en son bec un fromage.\")\n",
    "print(\"Tokens avec WordPiece :\", tokens_wordpiece)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
