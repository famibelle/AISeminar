{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk\n",
    "# %pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original :\n",
      " \n",
      "Maître Corbeau, sur un arbre perché,\n",
      "Tenait en son bec un fromage.\n",
      "Maître Renard, par l’odeur alléché,\n",
      "Lui tint à peu près ce langage :\n",
      "« Hé ! bonjour, Monsieur du Corbeau.\n",
      "Que vous êtes joli ! que vous me semblez beau !\n",
      "Sans mentir, si votre ramage\n",
      "Se rapporte à votre plumage,\n",
      "Vous êtes le Phénix des hôtes de ces bois. »\n",
      "A ces mots le Corbeau ne se sent pas de joie ;\n",
      "Et pour montrer sa belle voix,\n",
      "Il ouvre un large bec, laisse tomber sa proie.\n",
      "Le Renard s’en saisit, et dit : « Mon bon Monsieur,\n",
      "Apprenez que tout flatteur\n",
      "Vit aux dépens de celui qui l’écoute :\n",
      "Cette leçon vaut bien un fromage, sans doute. »\n",
      "Le Corbeau, honteux et confus,\n",
      "Jura, mais un peu tard, qu’on ne l’y prendrait plus.\n",
      "\n",
      "Jean de La Fontaine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemple de texte\n",
    "text = \"\"\"\n",
    "Maître Corbeau, sur un arbre perché,\n",
    "Tenait en son bec un fromage.\n",
    "Maître Renard, par l’odeur alléché,\n",
    "Lui tint à peu près ce langage :\n",
    "« Hé ! bonjour, Monsieur du Corbeau.\n",
    "Que vous êtes joli ! que vous me semblez beau !\n",
    "Sans mentir, si votre ramage\n",
    "Se rapporte à votre plumage,\n",
    "Vous êtes le Phénix des hôtes de ces bois. »\n",
    "A ces mots le Corbeau ne se sent pas de joie ;\n",
    "Et pour montrer sa belle voix,\n",
    "Il ouvre un large bec, laisse tomber sa proie.\n",
    "Le Renard s’en saisit, et dit : « Mon bon Monsieur,\n",
    "Apprenez que tout flatteur\n",
    "Vit aux dépens de celui qui l’écoute :\n",
    "Cette leçon vaut bien un fromage, sans doute. »\n",
    "Le Corbeau, honteux et confus,\n",
    "Jura, mais un peu tard, qu’on ne l’y prendrait plus.\n",
    "\n",
    "Jean de La Fontaine\n",
    "\"\"\"\n",
    "\n",
    "print(\"Texte original :\\n\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens avec NLTK : ['Maître', 'Corbeau', ',', 'sur', 'un', 'arbre', 'perché', ',', 'Tenait', 'en', 'son', 'bec', 'un', 'fromage', '.', 'Maître', 'Renard', ',', 'par', 'l', '’', 'odeur', 'alléché', ',', 'Lui', 'tint', 'à', 'peu', 'près', 'ce', 'langage', ':', '«', 'Hé', '!', 'bonjour', ',', 'Monsieur', 'du', 'Corbeau', '.', 'Que', 'vous', 'êtes', 'joli', '!', 'que', 'vous', 'me', 'semblez', 'beau', '!', 'Sans', 'mentir', ',', 'si', 'votre', 'ramage', 'Se', 'rapporte', 'à', 'votre', 'plumage', ',', 'Vous', 'êtes', 'le', 'Phénix', 'des', 'hôtes', 'de', 'ces', 'bois', '.', '»', 'A', 'ces', 'mots', 'le', 'Corbeau', 'ne', 'se', 'sent', 'pas', 'de', 'joie', ';', 'Et', 'pour', 'montrer', 'sa', 'belle', 'voix', ',', 'Il', 'ouvre', 'un', 'large', 'bec', ',', 'laisse', 'tomber', 'sa', 'proie', '.', 'Le', 'Renard', 's', '’', 'en', 'saisit', ',', 'et', 'dit', ':', '«', 'Mon', 'bon', 'Monsieur', ',', 'Apprenez', 'que', 'tout', 'flatteur', 'Vit', 'aux', 'dépens', 'de', 'celui', 'qui', 'l', '’', 'écoute', ':', 'Cette', 'leçon', 'vaut', 'bien', 'un', 'fromage', ',', 'sans', 'doute', '.', '»', 'Le', 'Corbeau', ',', 'honteux', 'et', 'confus', ',', 'Jura', ',', 'mais', 'un', 'peu', 'tard', ',', 'qu', '’', 'on', 'ne', 'l', '’', 'y', 'prendrait', 'plus', '.', 'Jean', 'de', 'La', 'Fontaine']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation avec NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens_nltk = word_tokenize(text)\n",
    "\n",
    "print(\"Tokens avec NLTK :\", tokens_nltk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : Tokenisation en sous-mots avec WordPiece (Hugging Face Tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens avec WordPiece : ['mai', '##tre', 'co', '##rb', '##eau', ',', 'sur', 'un', 'ar', '##bre', 'perch', '##e', ',', 'ten', '##ai', '##t', 'en', 'son', 'be', '##c', 'un', 'from', '##age', '.', 'mai', '##tre', 'ren', '##ard', ',', 'par', 'l', '’', 'ode', '##ur', 'all', '##eche', ',', 'lu', '##i', 'tin', '##t', 'a', 'pe', '##u', 'pre', '##s', 'ce', 'lang', '##age', ':', '«', 'he', '!', 'bon', '##jou', '##r', ',', 'monsieur', 'du', 'co', '##rb', '##eau', '.', 'que', 'vo', '##us', 'et', '##es', 'jo', '##li', '!', 'que', 'vo', '##us', 'me', 'se', '##mble', '##z', 'beau', '!', 'sans', 'men', '##ti', '##r', ',', 'si', 'vo', '##tre', 'rama', '##ge', 'se', 'rap', '##port', '##e', 'a', 'vo', '##tre', 'plumage', ',', 'vo', '##us', 'et', '##es', 'le', 'ph', '##eni', '##x', 'des', 'hot', '##es', 'de', 'ce', '##s', 'bois', '.', '»', 'a', 'ce', '##s', 'mo', '##ts', 'le', 'co', '##rb', '##eau', 'ne', 'se', 'sent', 'pas', 'de', 'jo', '##ie', ';', 'et', 'pour', 'mont', '##rer', 'sa', 'belle', 'vo', '##ix', ',', 'il', 'ou', '##vre', 'un', 'large', 'be', '##c', ',', 'lai', '##sse', 'tomb', '##er', 'sa', 'pro', '##ie', '.', 'le', 'ren', '##ard', 's', '’', 'en', 'sai', '##sit', ',', 'et', 'di', '##t', ':', '«', 'mon', 'bon', 'monsieur', ',', 'app', '##ren', '##ez', 'que', 'to', '##ut', 'flat', '##te', '##ur', 'vi', '##t', 'aux', 'de', '##pen', '##s', 'de', 'ce', '##lu', '##i', 'qui', 'l', '’', 'eco', '##ute', ':', 'ce', '##tte', 'le', '##con', 'va', '##ut', 'bien', 'un', 'from', '##age', ',', 'sans', 'do', '##ute', '.', '»', 'le', 'co', '##rb', '##eau', ',', 'hon', '##te', '##ux', 'et', 'con', '##fus', ',', 'ju', '##ra', ',', 'mai', '##s', 'un', 'pe', '##u', 'tar', '##d', ',', 'qu', '’', 'on', 'ne', 'l', '’', 'y', 'pre', '##ndra', '##it', 'plus', '.', 'jean', 'de', 'la', 'fontaine']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Charger un tokenizer pré-entraîné basé sur WordPiece (ex : BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenisation avec WordPiece\n",
    "tokens_wordpiece = tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Tokens avec WordPiece :\", tokens_wordpiece)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : Tokenisation par caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens (caractères) : ['\\n', 'M', 'a', 'î', 't', 'r', 'e', ' ', 'C', 'o', 'r', 'b', 'e', 'a', 'u', ',', ' ', 's', 'u', 'r', ' ', 'u', 'n', ' ', 'a', 'r', 'b', 'r', 'e', ' ', 'p', 'e', 'r', 'c', 'h', 'é', ',', '\\n', 'T', 'e', 'n', 'a', 'i', 't', ' ', 'e', 'n', ' ', 's', 'o']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation par caractères\n",
    "char_tokens = list(text)\n",
    "print(\"\\nTokens (caractères) :\", char_tokens[:50])  # Affiche les 50 premiers tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings avec Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Tokenization' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model_word2vec \u001b[38;5;241m=\u001b[39m Word2Vec(sentences, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Afficher le vecteur pour un mot\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mVecteur pour \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokenization\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmodel_word2vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTokenization\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/SourceCode/Lalux/.venv/lib/python3.11/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/SourceCode/Lalux/.venv/lib/python3.11/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/SourceCode/Lalux/.venv/lib/python3.11/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'Tokenization' not present\""
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Diviser le texte en phrases, puis en mots\n",
    "sentences = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "\n",
    "# Entraîner un modèle Word2Vec\n",
    "model_word2vec = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Afficher le vecteur pour un mot\n",
    "print(\"\\nVecteur pour 'Tokenization':\\n\", model_word2vec.wv['Tokenization'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver des mots similaires\n",
    "similar_words = model_word2vec.wv.most_similar(\"Tokenization\")\n",
    "print(\"\\nMots similaires à 'Tokenization' :\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Embeddings contextualisés avec BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Charger le modèle BERT et le tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Tokenisation et génération d'embeddings\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Embeddings de la séquence entière\n",
    "print(\"\\nVecteur d'embedding BERT (séquence entière) :\\n\", outputs.last_hidden_state.mean(dim=1).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire l'embedding pour un mot spécifique (indice 1 dans cet exemple)\n",
    "word_index = 1  # Indice du token \"Tokenization\"\n",
    "word_embedding = outputs.last_hidden_state[0, word_index, :]\n",
    "print(\"\\nEmbedding pour 'Tokenization' :\", word_embedding.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exemple avec Word2Vec\n",
    "words = list(model_word2vec.wv.index_to_key)[:10]\n",
    "vectors = [model_word2vec.wv[word] for word in words]\n",
    "\n",
    "# Réduction de dimensions\n",
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(vectors)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "for word, coord in zip(words, reduced_vectors):\n",
    "    plt.scatter(coord[0], coord[1])\n",
    "    plt.text(coord[0]+0.02, coord[1]+0.02, word)\n",
    "plt.title(\"Visualisation Word2Vec (PCA)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
