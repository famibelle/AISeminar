{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral-7b : Un guide pour le Fine-Tuning des LLMs\n",
    "Avec seulement 7 milliards de param√®tres, ces mod√®les sont un choix parfait pour les personnes souhaitant effectuer un fine-tuning des LLMs avec des exigences mat√©rielles limit√©es.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Efficient Fine Tuning : LoRA et Quantification\n",
    "Les mod√®les comme Mistral ont rendu possible le fine-tuning gratuitement. Malgr√© sa petite taille, il n√©cessite tout de m√™me jusqu'√† 30 Go de m√©moire GPU. Afin de r√©duire les exigences en mati√®re de m√©moire et les co√ªts, des techniques comme LoRA et la quantification sont utilis√©es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA\n",
    "LoRA est une technique d'adaptation de rang faible qui r√©duit le nombre de param√®tres pendant le fine-tuning. LoRA transforme la matrice de param√®tres en deux matrices de rang inf√©rieur, dont le produit scalaire approxime la matrice d'origine. Le rang \"r\" est ajustable, permettant de trouver un √©quilibre entre la vitesse et la qualit√© ‚Äî des rangs inf√©rieurs permettent un entra√Ænement plus rapide, mais au d√©triment de la qualit√©.\n",
    "\n",
    "### Quantification (QLoRA)\n",
    "Les LLMs utilisent g√©n√©ralement une pr√©cision sur 16 bits, ce qui prend beaucoup de m√©moire pour stocker des poids de haute pr√©cision. La quantification r√©duit la pr√©cision des poids √† 4 ou 8 bits, r√©duisant ainsi consid√©rablement la taille du mod√®le, au prix d'une l√©g√®re baisse de qualit√©. La quantification NF4 convertit les poids √† une pr√©cision de 4 bits. Ainsi, chaque poids peut prendre jusqu'√† 16 valeurs diff√©rentes, les normalisant √† une distribution centr√©e sur z√©ro, ce qui rend les mod√®les plus efficaces en termes de m√©moire tout en ayant un l√©ger impact sur la qualit√©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -U bitsandbytes\n",
    "# %pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# %pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# %pip install -q -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bitsandbytes\n",
    "# %pip install transformers\n",
    "# %pip install peft.git\n",
    "# %pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir l'appareil √† utiliser (GPU si disponible, sinon CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HUGGING_FACE_KEY =  os.environ.get(\"HuggingFace_API_KEY\")\n",
    "login(token=HUGGING_FACE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jpacifico/finetome_french_cook_definitions_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['system', 'question', 'answer'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir une fonction pour transformer chaque exemple\n",
    "def create_input_output(example):\n",
    "    example['input'] = f\"### Instruction: {example['system']} \\n ### Question: {example['question']}\"\n",
    "    example['output'] = example['answer']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"### Instruction: Tu es un assistant IA sp√©cialis√© dans le langage culinaire fran√ßais. Une question te sera pos√©e. Tu dois g√©n√©rer une r√©ponse pr√©cise et concise. \\n ### Question: Quelle est la d√©finition du mot 'Cr√©mer' en cuisine fran√ßaise ?\", 'output': \"En cuisine fran√ßaise, 'Cr√©mer' signifie ajouter de la cr√®me dans une pr√©paration ou m√©langer vigoureusement du sucre et du beurre pommade.\"}\n"
     ]
    }
   ],
   "source": [
    "# Appliquer la transformation sur chaque exemple du dataset\n",
    "new_dataset = dataset.map(create_input_output)\n",
    "\n",
    "# Supprimer les colonnes inutiles ('system', 'question', 'answers')\n",
    "new_dataset = new_dataset.remove_columns(['system', 'question', 'answer'])\n",
    "\n",
    "# Pour voir un exemple de ce nouveau dataset avec la colonne 'input'\n",
    "print(new_dataset['train'][0])  # Afficher le premier exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "organization = \"mistralai/\"\n",
    "model_name = \"Mistral-7B-v0.1\"\n",
    "base_model_id = organization + model_name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "max_length = 500 # Set an appropriate length for the dataset, default 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de formatage pour le prompt\n",
    "def format_func(input_text, output_text):\n",
    "    text = f\"### Instruction: {input_text}\\n ### Question: {output_text}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "# Fonction de tokenisation adapt√©e pour les batchs\n",
    "def tokenize_prompt_max_length(examples):\n",
    "    # Formatage du texte pour chaque exemple du batch\n",
    "    formatted_texts = [format_func(input_text, output_text) for input_text, output_text in zip(examples['input'], examples['output'])]\n",
    "    \n",
    "    # Tokenisation des textes format√©s en batch\n",
    "    result = tokenizer(\n",
    "        formatted_texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Cr√©er des labels bas√©s sur les input_ids pour chaque exemple\n",
    "    result[\"labels\"] = result[\"input_ids\"]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la fonction de tokenisation sur chaque split du dataset\n",
    "tokenized_dataset = new_dataset.map(tokenize_prompt_max_length, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"### Instruction: Tu es un assistant IA sp√©cialis√© dans le langage culinaire fran√ßais. Une question te sera pos√©e. Tu dois g√©n√©rer une r√©ponse pr√©cise et concise. \\n ### Question: Quelle est la d√©finition du mot 'Cr√©mer' en cuisine fran√ßaise ?\", 'output': \"En cuisine fran√ßaise, 'Cr√©mer' signifie ajouter de la cr√®me dans une pr√©paration ou m√©langer vigoureusement du sucre et du beurre pommade.\", 'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 774, 3133, 3112, 28747, 774, 3133, 3112, 28747, 16063, 1037, 521, 13892, 315, 28741, 27444, 4528, 17218, 2422, 462, 9848, 465, 13463, 1380, 536, 14299, 28723, 16035, 2996, 711, 26050, 977, 2110, 28723, 16063, 511, 278, 10979, 3160, 263, 2219, 3264, 1405, 18308, 864, 911, 3078, 864, 28723, 28705, 13, 774, 22478, 28747, 2332, 3222, 934, 543, 2306, 3013, 685, 1415, 2808, 464, 28743, 9946, 794, 28742, 481, 4014, 26704, 21801, 1550, 13, 774, 22478, 28747, 1618, 4014, 26704, 21801, 28725, 464, 28743, 9946, 794, 28742, 1492, 335, 412, 23371, 6753, 340, 543, 1439, 8158, 2422, 2219, 12823, 1389, 352, 3466, 290, 5560, 3689, 15068, 280, 267, 24862, 1415, 519, 961, 911, 1415, 347, 324, 267, 13296, 15248, 28723, 2], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 774, 3133, 3112, 28747, 774, 3133, 3112, 28747, 16063, 1037, 521, 13892, 315, 28741, 27444, 4528, 17218, 2422, 462, 9848, 465, 13463, 1380, 536, 14299, 28723, 16035, 2996, 711, 26050, 977, 2110, 28723, 16063, 511, 278, 10979, 3160, 263, 2219, 3264, 1405, 18308, 864, 911, 3078, 864, 28723, 28705, 13, 774, 22478, 28747, 2332, 3222, 934, 543, 2306, 3013, 685, 1415, 2808, 464, 28743, 9946, 794, 28742, 481, 4014, 26704, 21801, 1550, 13, 774, 22478, 28747, 1618, 4014, 26704, 21801, 28725, 464, 28743, 9946, 794, 28742, 1492, 335, 412, 23371, 6753, 340, 543, 1439, 8158, 2422, 2219, 12823, 1389, 352, 3466, 290, 5560, 3689, 15068, 280, 267, 24862, 1415, 519, 961, 911, 1415, 347, 324, 267, 13296, 15248, 28723, 2]}\n"
     ]
    }
   ],
   "source": [
    "# Pour voir un exemple du dataset tokenis√©\n",
    "print(tokenized_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser le dataset tokenis√© en splits train et √©valuation\n",
    "split_ratio = 0.1  # On prend 10% des donn√©es pour l'√©valuation\n",
    "train_test_split = tokenized_dataset['train'].train_test_split(test_size=split_ratio, seed=42)\n",
    "\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4500\n",
      "Eval dataset size: 500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"### Instruction: Tu es un assistant IA sp√©cialis√© dans le langage culinaire fran√ßais. Une question te sera pos√©e. Tu dois g√©n√©rer une r√©ponse pr√©cise et concise. \\n ### Question: Que signifie le terme 'Ch√¢trer' dans le contexte du langage culinaire fran√ßais ?\", 'output': 'Ch√¢trer signifie √©liminer le boyau central des √©crevisses avant leur cuisson.', 'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 774, 3133, 3112, 28747, 774, 3133, 3112, 28747, 16063, 1037, 521, 13892, 315, 28741, 27444, 4528, 17218, 2422, 462, 9848, 465, 13463, 1380, 536, 14299, 28723, 16035, 2996, 711, 26050, 977, 2110, 28723, 16063, 511, 278, 10979, 3160, 263, 2219, 3264, 1405, 18308, 864, 911, 3078, 864, 28723, 28705, 13, 774, 22478, 28747, 6018, 1492, 335, 412, 462, 1850, 28706, 464, 1209, 28891, 434, 263, 28742, 2422, 462, 379, 4690, 424, 1415, 9848, 465, 13463, 1380, 536, 14299, 1550, 13, 774, 22478, 28747, 689, 28891, 434, 263, 1492, 335, 412, 1233, 2788, 4828, 462, 4531, 581, 5971, 634, 1233, 961, 28728, 815, 274, 14221, 10441, 4014, 20947, 28723, 2], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 774, 3133, 3112, 28747, 774, 3133, 3112, 28747, 16063, 1037, 521, 13892, 315, 28741, 27444, 4528, 17218, 2422, 462, 9848, 465, 13463, 1380, 536, 14299, 28723, 16035, 2996, 711, 26050, 977, 2110, 28723, 16063, 511, 278, 10979, 3160, 263, 2219, 3264, 1405, 18308, 864, 911, 3078, 864, 28723, 28705, 13, 774, 22478, 28747, 6018, 1492, 335, 412, 462, 1850, 28706, 464, 1209, 28891, 434, 263, 28742, 2422, 462, 379, 4690, 424, 1415, 9848, 465, 13463, 1380, 536, 14299, 1550, 13, 774, 22478, 28747, 689, 28891, 434, 263, 1492, 335, 412, 1233, 2788, 4828, 462, 4531, 581, 5971, 634, 1233, 961, 28728, 815, 274, 14221, 10441, 4014, 20947, 28723, 2]}\n"
     ]
    }
   ],
   "source": [
    "# V√©rifier la structure d'un exemple\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous chargeons le mod√®le pr√©-entra√Æn√© en utilisant la biblioth√®que Transformers. Pour cela, nous initialisons le mod√®le avec une configuration de quantification sur 4 bits via `BitsAndBytesConfig`, ce qui r√©duit significativement l'utilisation de la m√©moire afin de s'assurer que le mod√®le puisse tenir dans notre session Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 14:59:24.537757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733234364.558209  146750 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733234364.564440  146750 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 14:59:24.585229: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca4a88e947a452b93b19d6c967430be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_length=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R√©ponse avec le mod√®le non fine tun√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©ponse: Que veut dire exactement 'Exprimer' dans le domaine culinaire ?–∂–µ –∫–∞–∫ –∏ –≤ –¥—Ä—É–≥–∏—Ö —Å—Ñ–µ—Ä–∞—Ö –∂–∏–∑–Ω–∏, –≤ –∫—É–ª–∏–Ω–∞—Ä–∏–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–µ—Ä–º–∏–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–ª–æ–∂–Ω—ã–º–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è —Ç–µ—Ö, –∫—Ç–æ —Ç–æ–ª—å–∫–æ –Ω–∞—á–∏–Ω–∞–µ—Ç –æ—Å–≤–∞–∏–≤–∞—Ç—å —ç—Ç–æ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –º–∏—Ä. –í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –º—ã –ø–æ–ø—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –≤—ã—Ä–∞–∂–µ–Ω–∏–µ \"exprimer\" –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è –ø–∏—â–∏, —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–≤ –µ–≥–æ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∏—Å—Ç–æ—Ä–∏—è, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Ä–µ—Ü–µ–ø—Ç–∞—Ö. –î–∞–≤–∞–π—Ç–µ –≤–º–µ—Å—Ç–µ –∏–∑—É—á–∏–º —ç—Ç–æ —Å–ª–æ–≤–æ, —á—Ç–æ–±—ã –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –æ–Ω–æ –≤–ª–∏—è–µ—Ç\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Que veut dire exactement 'Exprimer' dans le domaine culinaire ?\"\n",
    "# prompt = \"### Instruction: Tu es un assistant IA sp√©cialis√© dans la cuisine fran√ßaise. \\n ### Question: Quelle est la d√©finition du terme 'Exprimer' en cuisine fran√ßaise ?\"\n",
    "\n",
    "response = generate_response(prompt)\n",
    "print(\"R√©ponse:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√Ä ce stade, nous configurons LoRA avec \"r=16\" et \"alpha=64\" afin de trouver un √©quilibre entre efficacit√© et pr√©cision lors du fine-tuning. Le param√®tre \"r\" ajuste le rang de la matrice de faible rang : des valeurs plus √©lev√©es offrent de meilleures performances, mais au prix d'un fine-tuning plus lent. Le param√®tre \"alpha\" permet de moduler les poids, des valeurs plus √©lev√©es donnant davantage d'importance aux activations de LoRA. Ce morceau de code affiche le pourcentage de param√®tres entra√Ænables pendant le fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_train_params(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 42520576 || all params: 3794591744 || trainable%: 1.1205573318192514\n"
     ]
    }
   ],
   "source": [
    "print_train_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous proc√©dons au fine-tuning du mod√®le, en d√©finissant `max_steps` √† 500 par exemple et un `learning_rate` de 1e-4. \n",
    "\n",
    "Nous configurons le `Trainer` pour √©valuer le mod√®le tous les 25 pas, ce qui nous permet de suivre la progression de l'apprentissage. \n",
    "\n",
    "En utilisant le param√®tre `save_strategy`, nous sauvegardons les poids du mod√®le √† chaque point d'√©valuation, c'est-√†-dire tous les 25 pas. \n",
    "\n",
    "Cette strat√©gie nous permettra plus tard de s√©lectionner les poids en fonction des performances d'√©valuation. De cette mani√®re, nous pourrons potentiellement faire face au surapprentissage (overfitting) en choisissant des poids obtenus lors d'une phase ant√©rieure de l'entra√Ænement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"journal-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"/mnt/Models/Mistral7B_finetuned_\" + run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/SourceCode/Lalux/.venv/lib/python3.11/site-packages/transformers/training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset  = eval_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=500, # default 500\n",
    "        learning_rate=1e-4,\n",
    "        fp16=True,\n",
    "        fp16_full_eval=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=20,\n",
    "        logging_dir=\"./logs\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50, #125 default\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50, #125 default\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmedhi-famibelle\u001b[0m (\u001b[33mmedhi-famibelle-akabi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/azureuser/SourceCode/Lalux/wandb/run-20241203_150127-x27hesyi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/medhi-famibelle-akabi/huggingface/runs/x27hesyi' target=\"_blank\">/mnt/Models/Mistral7B_finetuned_mistral-journal-finetune</a></strong> to <a href='https://wandb.ai/medhi-famibelle-akabi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/medhi-famibelle-akabi/huggingface' target=\"_blank\">https://wandb.ai/medhi-famibelle-akabi/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/medhi-famibelle-akabi/huggingface/runs/x27hesyi' target=\"_blank\">https://wandb.ai/medhi-famibelle-akabi/huggingface/runs/x27hesyi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 1:16:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.556700</td>\n",
       "      <td>0.552925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.462700</td>\n",
       "      <td>0.517624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.459600</td>\n",
       "      <td>0.467096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.443500</td>\n",
       "      <td>0.394231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.343100</td>\n",
       "      <td>0.373917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>0.341620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>0.317975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.308100</td>\n",
       "      <td>0.299921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.289100</td>\n",
       "      <td>0.290728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/SourceCode/Lalux/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/azureuser/SourceCode/Lalux/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/azureuser/SourceCode/Lalux/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/azureuser/SourceCode/Lalux/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/azureuser/SourceCode/Lalux/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/azureuser/SourceCode/Lalux/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/azureuser/SourceCode/Lalux/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/azureuser/SourceCode/Lalux/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/azureuser/SourceCode/Lalux/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/azureuser/SourceCode/Lalux/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.4199739284515381, metrics={'train_runtime': 4621.0347, 'train_samples_per_second': 0.216, 'train_steps_per_second': 0.108, 'total_flos': 2.1459542016e+16, 'train_loss': 0.4199739284515381, 'epoch': 0.2222222222222222})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous allons recharger le mod√®le dans la session, il n'y aura pas assez de m√©moire GPU pour h√©berger le mod√®le de base ainsi que le mod√®le fine-tun√©. Par cons√©quent, nous recommandons de sauvegarder les poids sur votre ordinateur. Les poids peuvent √™tre trouv√©s √† l'adresse : `content/mistral-journal-finetune/checkpoint-xxx/`. Ensuite, red√©marrez le kernel et chargez √† nouveau les poids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61139b45e6504e3ead4af1aacedc7d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "organization = \"mistralai/\"\n",
    "model_name = \"Mistral-7B-v0.1\"\n",
    "base_model_id = organization + model_name\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_model = PeftModel.from_pretrained(base_model, \"/content/mistral-journal-finetune/checkpoint-750\")\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"/mnt/Models/Mistral7B_finetuned_mistral-journal-finetune/checkpoint-450/\")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous pouvons ex√©cuter des inf√©rences sur notre mod√®le fine-tun√©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemple1 = \"Quelle est la d√©finition du mot 'Cr√©mer' en cuisine fran√ßaise ?\"\n",
    "exemple2 = \"Que veut dire exactement 'Exprimer' dans le domaine culinaire ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quelle est la d√©finition du mot 'Cr√©mer' en cuisine fran√ßaise ?\n",
      " ### Cr√©mer : Ajouter de la cr√®me dans une pr√©paration.\n",
      "\n",
      "Quelle est la d√©finition du mot 'Dess√©cher' en cuisine fran√ßaise ?\n",
      " ### Dess√©cher : Rendre un aliment moins sec en le plongeant dans un liqu\n"
     ]
    }
   ],
   "source": [
    "model_input = eval_tokenizer(exemple1, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=64, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que veut dire exactement 'Exprimer' dans le domaine culinaire ?\n",
      " ### Exprimer signifie faire sortir du jus des fruits ou des l√©gumes en les pressant. Cela peut √©galement d√©signer l'action de d√©crire une recette √† haute voix, comme un chef d'un restaurant. En cuisine, exprimer est syn\n"
     ]
    }
   ],
   "source": [
    "model_input = eval_tokenizer(exemple2, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=64, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
